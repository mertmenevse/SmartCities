# -*- coding: utf-8 -*-
"""smartCities

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zFGHvLvCXTJcNBaLnkZx_nQzOVBn08AM
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras.applications import InceptionV3, VGG16
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GRU, GlobalAveragePooling2D, Reshape, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_curve, auc, f1_score, accuracy_score, recall_score, precision_score
import matplotlib.pyplot as plt
import seaborn as sns
import os

# CLAHE ile Görüntü İşleme
def preprocess_image(image_path):
    image = cv2.imread(image_path)
    if image is None:
        print(f"Hata: {image_path} konumundaki görüntü okunamadı.")
        return None
    image = cv2.resize(image, (224, 224))
    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    image = clahe.apply(image)
    image = np.stack([image, image, image], axis=-1) / 255.0
    return image

# Veriyi yükleme ve etiketleme
def load_data(base_dir):
    classes = ['Dense', 'Medium_Dense', 'Sparse', 'None']
    image_paths, labels = [], []
    for cls in classes:
        class_dir = os.path.join(base_dir, cls)
        for img_name in os.listdir(class_dir):
            img_path = os.path.join(class_dir, img_name)
            processed_image = preprocess_image(img_path)
            if processed_image is not None:
                image_paths.append(processed_image)
                labels.append(cls)
    data = np.array(image_paths)
    le = LabelEncoder()
    labels = le.fit_transform(labels)
    return data, labels

# Hiperparametre Optimizasyonu için MPA Algoritması
def MPA(population_size, max_iter, search_space):
    pop = np.random.uniform([param[0] for param in search_space], [param[1] for param in search_space], (population_size, len(search_space)))
    fitness = np.array([objective_function(ind) for ind in pop])
    best_solution = pop[np.argmin(fitness)]
    best_fitness = np.min(fitness)

    for t in range(max_iter):
        for i in range(population_size):
            predator_effect = np.random.uniform(-1, 1, size=len(search_space)) * (best_solution - pop[i])
            pop[i] += predator_effect
            pop[i] = np.clip(pop[i], [param[0] for param in search_space], [param[1] for param in search_space])
            current_fitness = objective_function(pop[i])
            if current_fitness < fitness[i]:
                fitness[i] = current_fitness
            if current_fitness < best_fitness:
                best_solution = pop[i]
                best_fitness = current_fitness
    return best_solution

# CSTOA Algoritması
def CSTOA(population_size, max_iter, search_space):
    pop = np.random.uniform([param[0] for param in search_space], [param[1] for param in search_space], (population_size, len(search_space)))
    fitness = np.array([objective_function(ind) for ind in pop])
    best_solution = pop[np.argmin(fitness)]
    best_fitness = np.min(fitness)

    chaotic_map = np.random.rand(population_size, len(search_space))
    for t in range(max_iter):
        for i in range(population_size):
            if np.random.rand() < 0.5:
                chaotic_map[i] = chaotic_map[i] * np.sin(2 * np.pi * chaotic_map[i])
            else:
                chaotic_map[i] = chaotic_map[i] * np.cos(2 * np.pi * chaotic_map[i])
            new_solution = best_solution + chaotic_map[i] * (pop[i] - best_solution)
            new_solution = np.clip(new_solution, [param[0] for param in search_space], [param[1] for param in search_space])
            if np.random.rand() > 0.5:
                new_solution += np.random.uniform(-0.1, 0.1, size=len(search_space[0]))
            current_fitness = objective_function(new_solution)
            if current_fitness < fitness[i]:
                pop[i] = new_solution
                fitness[i] = current_fitness
            if current_fitness < best_fitness:
                best_solution = new_solution
                best_fitness = current_fitness
    return best_solution

# Ölçüt fonksiyonu (Objective function)
def objective_function(params):
    return np.sum(params**2)

# Model Oluşturma (InceptionV3 ve GRU ile)
def build_model(hyperparams):
    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    base_model.trainable = False
    model = Sequential([
        base_model,
        GlobalAveragePooling2D(),
        Dense(int(hyperparams[0]), activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Reshape((1, int(hyperparams[0]))),
        GRU(int(hyperparams[1]), return_sequences=True),
        GRU(int(hyperparams[2])),
        Dense(4, activation='softmax')
    ])
    model.compile(optimizer=Adam(learning_rate=hyperparams[3]), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Model eğitimi ve veri artırma
def train_model(model, data, labels, epochs=500, batch_size=64):
    X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)
    datagen = ImageDataGenerator(
        rotation_range=40,
        width_shift_range=0.4,
        height_shift_range=0.4,
        zoom_range=0.4,
        horizontal_flip=True,
        fill_mode='nearest'
    )
    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
    checkpoint = ModelCheckpoint('model_best.keras', save_best_only=True, monitor='val_loss', mode='min')
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=1e-6)
    history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),
                        epochs=epochs,
                        validation_data=(X_val, y_val),
                        callbacks=[early_stopping, checkpoint, reduce_lr])
    return model, history

# Performans Karşılaştırmaları için GoogleNet ve VGGNet ile Modellerin Eğitimi
def train_alternate_models(X_train, y_train, X_val, y_val, epochs=100):
    models = {'GoogleNet': VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))}
    histories = {}
    for name, base_model in models.items():
        base_model.trainable = False
        model = Sequential([
            base_model,
            GlobalAveragePooling2D(),
            Dense(256, activation='relu'),
            BatchNormalization(),
            Dropout(0.5),
            Dense(4, activation='softmax')
        ])
        model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), verbose=1)
        histories[name] = history
    return histories

# G-Measure Hesaplama
def g_measure(precision, recall):
    return 2 * ((precision * recall) / (precision + recall))

# Eğitim Sonuçlarının Çizdirilmesi ve G-Measure Ekleme
def plot_metrics(history, y_true, y_pred):
    f1 = f1_score(y_true, y_pred, average='weighted')
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    gmeasure = g_measure(precision, recall)

    print(f"F1 Score: {f1}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"G-Measure: {gmeasure}")

    plt.figure(figsize=(12, 8))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label="Eğitim Doğruluğu")
    plt.plot(history.history['val_accuracy'], label="Doğrulama Doğruluğu")
    plt.xlabel("Epochs")
    plt.ylabel("Doğruluk")
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label="Eğitim Kaybı")
    plt.plot(history.history['val_loss'], label="Doğrulama Kaybı")
    plt.xlabel("Epochs")
    plt.ylabel("Kayıp")
    plt.legend()
    plt.show()

# Ana Program
if __name__ == '__main__':
    base_dir = '/content/drive/MyDrive/Colab Notebooks/dataset/'
    data, labels = load_data(base_dir)
    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

    # MPA ve CSTOA ile Hiperparametre Optimizasyonu
    mpa_params = MPA(population_size=10, max_iter=50, search_space=[[64, 512], [32, 128], [32, 128], [0.0001, 0.01]])
    print("En İyi Hiperparametreler (MPA):", mpa_params)

    # Modelin Eğitimi
    model = build_model(mpa_params)
    model, history = train_model(model, data, labels)

    # Karışıklık Matrisi ve Metriklerin Çizdirilmesi
    y_pred = np.argmax(model.predict(X_test), axis=1)
    plot_metrics(history, y_test, y_pred)

    # Diğer Modeller ile Karşılaştırma
    alternate_histories = train_alternate_models(X_train, y_train, X_test, y_test)